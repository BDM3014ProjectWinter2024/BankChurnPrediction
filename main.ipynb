{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas sagemaker boto3 botocore numpy matplotlib seaborn scikit-learn nbconvert\n",
    "# !pip install imbalanced-learn lime shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for S3 connection\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import botocore\n",
    "from io import StringIO\n",
    "\n",
    "# Import packages for data manipulation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import packages for data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Import packages for featured engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import packages for data modeling\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
    "f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "#SVM\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "# This lets us see all of the columns, preventing Juptyer from redacting them.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# This module lets us save our models once we fit them.\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Conection to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3conn.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # Set the path to S3 AWS credentials file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Using credentials from a file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m s3_utils \u001b[38;5;241m=\u001b[39m \u001b[43mS3Utils\u001b[49m\u001b[43m(\u001b[49m\u001b[43msecret_name_or_arn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Usersadria\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;130;43;01m\\b\u001b[39;49;00m\u001b[38;5;124;43mucketcredentials.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize S3Utils using credentials from Secrets Manager\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#s3_utils = S3Utils(secret_name_or_arn=\"arn:aws:secretsmanager:us-east-2:767397996410:secret:dev/s3/bucket_token-6t6xMP\")\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define your parameters\u001b[39;00m\n\u001b[0;32m     13\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#  dev, test, staging, prod \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3032\\3532942846.py:6\u001b[0m, in \u001b[0;36mS3Utils.__init__\u001b[1;34m(self, secret_name_or_arn, file_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m file_path\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maws_access_key_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maws_secret_access_key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_aws_credentials_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maws_access_key_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maws_secret_access_key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_aws_credentials_from_secrets_manager()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3032\\3532942846.py:18\u001b[0m, in \u001b[0;36mS3Utils.get_aws_credentials_from_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_aws_credentials_from_file\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 18\u001b[0m     credentials_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m credentials_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maws_access_key_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m], credentials_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maws_secret_access_key\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m], credentials_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbucket_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the S3Utils class from s3conn.ipynb\n",
    "%run s3conn.ipynb\n",
    "\n",
    "# # Set the path to S3 AWS credentials file\n",
    "# Using credentials from a file\n",
    "s3_utils = S3Utils(secret_name_or_arn=None, file_path=\"C:/Users/adria/Downloads/bucketcredentials.csv\")\n",
    "\n",
    "# Initialize S3Utils using credentials from Secrets Manager\n",
    "#s3_utils = S3Utils(secret_name_or_arn=\"arn:aws:secretsmanager:us-east-2:767397996410:secret:dev/s3/bucket_token-6t6xMP\")\n",
    "\n",
    "\n",
    "# Define your parameters\n",
    "env = \"dev\" #  dev, test, staging, prod \n",
    "mainsource = \"SourceDataSet/bank_data_train.csv\"\n",
    "envraw = f'{env}/raw/bank_data_train.csv'\n",
    "\n",
    "# Always check the Raw SourceCode is it exists if that it will copy the file to raw on a specific environment\n",
    "# Send sourcefile to env/raw\n",
    "s3_utils.check_and_transfer_file(mainsource, envraw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data from the S3 bucket\n",
    "input_file_key_data_cleaning = f'{env}/raw/bank_data_train.csv'  #this filename is from Kaggle and for the purpose of identification we are going to use the same filename as our main raw datasources. subfolder are used for distinction of file\n",
    "\n",
    "# Output file key directory which is also servers as an input for other process\n",
    "output_file_key_data_cleaning = f'{env}/processed/bank_data_cleaned.csv'\n",
    "\n",
    "output_file_key_data_visualization = f'{env}/processed/bank_data_visualization.csv'\n",
    "\n",
    "output_file_key_data_feature_engineering= f'{env}/final/bank_data_feature_eng.csv'\n",
    "\n",
    "output_file_key_data_X_train = f'{env}/final/model_one/X_train.csv'\n",
    "output_file_key_data_X_test = f'{env}/final/model_one/X_test.csv'\n",
    "\n",
    "output_file_key_data_Y_train = f'{env}/final/model_one/Y_train.csv'\n",
    "output_file_key_data_Y_test = f'{env}/final/model_one/Y_test.csv'\n",
    "\n",
    "# output_file_key_data_model_one_train = f'{env}/final/model_one/bank_data_train.csv'\n",
    "# output_file_key_data_model_one_test = f'{env}/final/model_one/bank_data_test.csv'\n",
    "\n",
    "# output_file_key_data_model_two_train =   f'{env}/final/model_two/bank_data_train.csv'\n",
    "# output_file_key_data_model_two_test =   f'{env}/final/model_two/bank_data_test.csv'\n",
    "\n",
    "output_file_key_data_model_three_train = f'{env}/final/model_c/bank_data_train.csv'\n",
    "output_file_key_data_model_three_test = f'{env}/final/model_c/bank_data_test.csv'\n",
    "output_file_key_data_svm_model_pkl = f'{env}/final/model_c/model.pkl'\n",
    "output_file_key_data_svm_model_tar = f'{env}/final/model_c/model.tar.gz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute notebooks accdly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run data_cleaning.ipynb\n",
    "#%run data_visualization.ipynb\n",
    "#%run feature_engineering.ipynb\n",
    "#%run random_forest.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
