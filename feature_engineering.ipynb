{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below if you want to run this file only\n",
    "#%run main.ipynb\n",
    "#%run data_cleaning.ipynb\n",
    "#%run data_visualization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV VISUALIZATION data source file from S3 into a DataFrame\n",
    "# Use the methods from the S3Utils class\n",
    "if s3_utils.check_file_exists(output_file_key_data_visualization):\n",
    "    df = s3_utils.read_csv_from_s3(output_file_key_data_visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Encoding categorical variables into numerical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the tag encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#Applying the encoding to the \"pack\" column:\n",
    "df['pack_encoded'] = label_encoder.fit_transform(df['pack'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pack_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping pack and status features\n",
    "df.drop(columns=['pack'], inplace=True)\n",
    "df.drop(columns=['status'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove or transform irrelevant or redundant features to streamline the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_products'] = df['cr_prod_cnt_il'].fillna(0) + df['cr_prod_cnt_vcu'].fillna(0) + df['cr_prod_cnt_tovr'].fillna(0) + df['cr_prod_cnt_pil'].fillna(0) + df['cr_prod_cnt_cc'].fillna(0) + df['cr_prod_cnt_ccfp'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_products'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TARGET (Dependent Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of classes of dependent variable\n",
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customers leaving the bank\n",
    "churn = df.loc[df[\"target\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customers who did not leave the bank\n",
    "not_churn = df.loc[df[\"target\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of not_churn group according to Months of being customers\n",
    "not_churn[\"clnt_setup_tenor\"].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of churn group according to Months of being customers\n",
    "churn[\"clnt_setup_tenor\"].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# examining the age of the not_churn group\n",
    "not_churn[\"age\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the Age for not_churn\n",
    "pyplot.figure(figsize=(8,6))\n",
    "pyplot.xlabel('Age')\n",
    "pyplot.hist(not_churn[\"age\"],bins=15, alpha=0.7, label='Not Churn')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining the age of the churn group\n",
    "churn[\"age\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the Age for churn\n",
    "pyplot.figure(figsize=(8,6))\n",
    "pyplot.xlabel('Age')\n",
    "pyplot.hist(churn[\"age\"],bins=15, alpha=0.7, label='Churn')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='target', y='age', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discretizing age feature \n",
    "\n",
    "# Defining category limits\n",
    "bins = [float('-inf'), 19, 56, float('inf')]  # Categories: (-inf, 19], (19, 56], (56, inf)\n",
    "\n",
    "# Defining labels\n",
    "labels = ['Under 20', '20-55 years', 'Over 55']\n",
    "\n",
    "# Creating a new feature with the categories created \n",
    "df['new_age_category'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Assigning numerical values to categories\n",
    "category_mapping = {'Under 20': 0, '20-55 years': 1, 'Over 55': 2}\n",
    "df['new_age_category_numeric'] = df['new_age_category'].map(category_mapping)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['new_age_category'].unique())\n",
    "print(df['new_age_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of not_churn group according to NumOfProducts\n",
    "not_churn[\"number_products\"].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Frequency of not_churn group according to NumOfProducts\n",
    "not_churn[\"number_products\"].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of churn group according to NumOfProducts\n",
    "churn[\"number_products\"].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Standarization and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clnt_setup_tenor_years'] = df['clnt_setup_tenor'] * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing tenure in relation to age, potentially offering a fresh perspective on customer retention based on age.\n",
    "df[\"new_setup_tenor\"] = df[\"clnt_setup_tenor_years\"]/df[\"age\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping more redundant features \n",
    "\n",
    "#age and new setup tenor in years\n",
    "df.drop(['age', 'clnt_setup_tenor_years', 'clnt_setup_tenor'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_columns = [col for col in df.columns if 'age' in col]\n",
    "print(age_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping age_group \n",
    "df.drop(['age_group'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking for columns with the word cnt \n",
    "cnt_columns = [col for col in df.columns if 'cnt' in col]\n",
    "print(cnt_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns of individual products\n",
    "df.drop(['cr_prod_cnt_il', 'cr_prod_cnt_vcu', 'cr_prod_cnt_tovr', 'cr_prod_cnt_pil', 'cr_prod_cnt_cc', 'cr_prod_cnt_ccfp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for columns with the word turnover to sum later  \n",
    "turnover_columns = [col for col in df.columns if 'turnover' in col]\n",
    "\n",
    "# Printing results\n",
    "print(turnover_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the values of all turnover columns\n",
    "df['total_turnover'] = df[['turnover_cc', 'turnover_paym']].sum(axis=1)\n",
    "\n",
    "print(df[['total_turnover']].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizing the new total_turnover feature using pd.qcut() with 'duplicates' set to 'drop'\n",
    "df['total_turnover_category'] = pd.qcut(df['total_turnover'], 6, labels=False, duplicates='drop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'total_turnover' column from the DataFrame\n",
    "df.drop(columns=['total_turnover', 'turnover_cc', 'turnover_paym'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking for columns related to 1m  \n",
    "one_month_columns = [col for col in df.columns if '1m' in col]\n",
    "print(one_month_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the values of all turnover columns related to dynamic in one month \n",
    "df['total_turnover_1m'] = df[['turnover_dynamic_il_1m', 'turnover_dynamic_cur_1m', 'turnover_dynamic_paym_1m','turnover_dynamic_cc_1m' ]].sum(axis=1)\n",
    "\n",
    "print(df[['total_turnover_1m']].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizing the new total_turnover_1m feature using pd.qcut() with 'duplicates' set to 'drop'\n",
    "df['total_turnover_1m_category'] = pd.qcut(df['total_turnover_1m'], 6, labels=False, duplicates='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'total_turnover' column from the DataFrame related to 1m\n",
    "df.drop(columns=['turnover_dynamic_il_1m', 'turnover_dynamic_cur_1m', 'turnover_dynamic_paym_1m','turnover_dynamic_cc_1m', 'total_turnover_1m' ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking for columns related to 3 months\n",
    "three_months_columns = [col for col in df.columns if '3m' in col]\n",
    "print(three_months_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the values of all turnover columns related to dynamic in three months \n",
    "df['total_turnover_3m'] = df[['turnover_dynamic_il_3m', 'turnover_dynamic_cur_3m', 'turnover_dynamic_paym_3m','turnover_dynamic_cc_3m' ]].sum(axis=1)\n",
    "\n",
    "print(df[['total_turnover_3m']].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizing the new total_turnover_3m feature using pd.qcut() with 'duplicates' set to 'drop'\n",
    "df['total_turnover_3m_category'] = pd.qcut(df['total_turnover_3m'], 6, labels=False, duplicates='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'total_turnover' column from the DataFrame related to three months\n",
    "df.drop(columns=['turnover_dynamic_il_3m', 'turnover_dynamic_cur_3m', 'turnover_dynamic_paym_3m','turnover_dynamic_cc_3m', 'total_turnover_3m', 'total_turnover_3m' ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns =[]\n",
    "cat_columns=[]\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        cat_columns.append(col)\n",
    "    else:\n",
    "        numeric_columns.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZING HEATMAP TO FIND MORE RELATED FEATURES TO DROP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding new_age_category column\n",
    "numeric_columns_without_new_age = [col for col in df[numeric_columns].columns if col != 'new_age_category']\n",
    "\n",
    "# Calculating correlation\n",
    "correlation_matrix = df[numeric_columns_without_new_age].corr()\n",
    "\n",
    "# Setting the figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Visualizing heatmap \n",
    "sns.heatmap(correlation_matrix, cmap='Blues', linewidths=1, annot=True, annot_kws={\"size\": 8}, fmt=\".2f\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is decided based on the heatmap to eliminate the columns: 'trans_count_nas_prc', 'trans_count_sup_prc', 'trans_count_atm_prc'.\n",
    "\n",
    "Likewise, it was determined to eliminate the columns related to 'rest_dynamic' that describe information for 1 month, these are:\n",
    "'rest_dynamic_fdep_1m', 'rest_dynamic_il_1m', 'rest_dynamic_cur_1m', 'rest_dynamic_paym_1m', 'rest_dynamic_cc_1m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping redundant feature finding in the heatmap \n",
    "df.drop(columns=['trans_count_nas_prc', 'new_age_category','trans_count_sup_prc', 'trans_count_atm_prc','rest_dynamic_fdep_1m', 'rest_dynamic_il_1m', 'rest_dynamic_cur_1m', 'rest_dynamic_paym_1m', 'rest_dynamic_cc_1m'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Standarizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling features using statistics that are robust to outliers.\n",
    "\n",
    "def robust_scaler(variable):\n",
    "    if variable.dtype == 'object':  # If the variable is of type 'object', it's likely a label encoded column\n",
    "        return variable\n",
    "    else:  # If the variable is numeric, apply robust scaling\n",
    "        var_median = variable.median()\n",
    "        quartile1 = variable.quantile(0.25)\n",
    "        quartile3 = variable.quantile(0.75)\n",
    "        interquantile_range = quartile3 - quartile1\n",
    "        if int(interquantile_range) == 0:\n",
    "            quartile1 = variable.quantile(0.05)\n",
    "            quartile3 = variable.quantile(0.95)\n",
    "            interquantile_range = quartile3 - quartile1\n",
    "            if int(interquantile_range) == 0:\n",
    "                quartile1 = variable.quantile(0.01)\n",
    "                quartile3 = variable.quantile(0.99)\n",
    "                interquantile_range = quartile3 - quartile1\n",
    "                z = (variable - var_median) / interquantile_range\n",
    "                return round(z, 3)\n",
    "\n",
    "            z = (variable - var_median) / interquantile_range\n",
    "            return round(z, 3)\n",
    "        else:\n",
    "            z = (variable - var_median) / interquantile_range\n",
    "        return round(z, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Featured Engineering\n",
    "#### send df to S3 for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Write the analyzed data to S3\n",
    "s3_utils.write_csv_to_s3(output_file_key_data_feature_engineering, df)\n",
    "print(f\"File '{output_file_key_data_feature_engineering}' successfully written to bucket '{s3_utils.bucket_name}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
