{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing evaluation by using XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+--------------------------------------------------------+\n",
      "| Metric    |   Score with XGBoost Classifier |   Score with XGBoost Classifier using Random Search CV |\n",
      "+===========+=================================+========================================================+\n",
      "| ROC Curve |                        0.7635   |                                              0.81      |\n",
      "+-----------+---------------------------------+--------------------------------------------------------+\n",
      "| F1 Score  |                        0.275313 |                                              0.0318564 |\n",
      "+-----------+---------------------------------+--------------------------------------------------------+\n",
      "| Recall    |                        0.371122 |                                              0.0163764 |\n",
      "+-----------+---------------------------------+--------------------------------------------------------+\n",
      "| Precision |                        0.218822 |                                              0.581967  |\n",
      "+-----------+---------------------------------+--------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Define the metrics and corresponding scores for the initial XGBoost classifier\n",
    "metrics_original = ['Precision', 'Recall', 'F1 Score','ROC Curve']\n",
    "scores_original = [0.21882224942200462, 0.3711221312420713, 0.27531334217393166,0.7635]\n",
    "\n",
    "# Define the metrics and corresponding scores for the new set of metrics\n",
    "metrics_new = ['Precision', 'Recall', 'F1 Score','ROC Curve']\n",
    "scores_new = [0.5819672131147541, 0.01637642717102987, 0.031856421761076836,0.81]\n",
    "\n",
    "# Create a list of lists containing metric and score pairs for both sets of metrics\n",
    "table_data = [\n",
    "    ['Metric', 'Score with XGBoost Classifier', 'Score with XGBoost Classifier using Random Search CV'],\n",
    "    *zip(metrics_original[::-1], scores_original[::-1], scores_new[::-1])\n",
    "]\n",
    "\n",
    "# Print the comparison table using tabulate\n",
    "table_str = tabulate(table_data, headers='firstrow', tablefmt='grid')\n",
    "\n",
    "print(table_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Score with XGBoost Classifier appears to be a better choice for predicting customer churn based on the metrics, especially if your priority is to accurately identify churned customers (higher recall) while maintaining a reasonable level of precision.\n",
    "\n",
    ">Score with XGBoost Classifier using RandomSearch CV shows higher precision but at the expense of recall, which might lead to missing many actual churn cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing evaluation by using Linear SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------+\n",
      "| Metric    |   Score With SVM Model using LinearSVC |\n",
      "+===========+========================================+\n",
      "| Precision |                                 0.1278 |\n",
      "+-----------+----------------------------------------+\n",
      "| Recall    |                                 0.7146 |\n",
      "+-----------+----------------------------------------+\n",
      "| F1 Score  |                                 0.2168 |\n",
      "+-----------+----------------------------------------+\n",
      "| AUC-ROC   |                                 0.6412 |\n",
      "+-----------+----------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Define the metrics and corresponding scores for the LinearSVC model\n",
    "metrics = ['Precision', 'Recall', 'F1 Score', 'AUC-ROC']\n",
    "scores = [0.1278, 0.7146, 0.2168, 0.6412]\n",
    "\n",
    "# Create a list of lists containing metric and score pairs\n",
    "table_data = list(zip(metrics, scores))\n",
    "\n",
    "# Print the table using tabulate\n",
    "table_str = tabulate(table_data, headers=['Metric', 'Score With SVM Model using LinearSVC'], tablefmt='grid')\n",
    "\n",
    "print(table_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing evaluation by using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------+-------------------------------------------------+\n",
      "| Metric    |   Random Forest (Random Search CV) |   Random Forest (Random Search CV in SageMaker) |\n",
      "+===========+====================================+=================================================+\n",
      "| Precision |                        1           |                                        0.984746 |\n",
      "+-----------+------------------------------------+-------------------------------------------------+\n",
      "| Recall    |                        0.000691962 |                                        0.917753 |\n",
      "+-----------+------------------------------------+-------------------------------------------------+\n",
      "| F1 Score  |                        0.00138297  |                                        0.95007  |\n",
      "+-----------+------------------------------------+-------------------------------------------------+\n",
      "| ROC AUC   |                        0.500346    |                                        0.951742 |\n",
      "+-----------+------------------------------------+-------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Define the metrics and corresponding scores for the first Random Forest model\n",
    "metrics_rf1 = ['Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "scores_rf1 = [1.0, 0.0006919617114519663, 0.0013829664630632707, 0.500345980855726]\n",
    "\n",
    "# Define the metrics and corresponding scores for the second Random Forest model\n",
    "metrics_rf2 = ['Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "scores_rf2 = [0.9847461892829396, 0.9177527585714577, 0.9500699411438676, 0.9517423683748888]\n",
    "\n",
    "# Create a list of lists containing metric and score pairs for both Random Forest models\n",
    "table_data = [\n",
    "    ['Metric', 'Random Forest (Random Search CV)', 'Random Forest (Random Search CV in SageMaker)'],\n",
    "    *zip(metrics_rf1, scores_rf1, scores_rf2)\n",
    "]\n",
    "\n",
    "# Print the table using tabulate\n",
    "table_str = tabulate(table_data, headers='firstrow', tablefmt='grid')\n",
    "\n",
    "print(table_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "The Random Forest model using Random Search CV in SageMaker performs significantly better across all metrics compared to the other model:\n",
    "It has very high precision (0.9847), indicating a low rate of false positives.\n",
    "It also has a high recall (0.9178), indicating that it captures a large portion of actual churners.\n",
    "The F1 Score (0.9501) reflects a good balance between precision and recall.\n",
    "The ROC AUC (0.9517) is also high, suggesting strong discriminative ability.\n",
    "\n",
    "### Conclusion:\n",
    "Based on the provided metrics, the Random Forest model using Random Search CV in SageMaker is preferable for customer churn prediction. It achieves a good balance between precision and recall, with high overall performance indicated by the ROC AUC score. This model is likely better suited for accurately identifying customers who are likely to churn, which is crucial for effective churn prediction and retention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERALL CONCLUSION:\n",
    ">For churn prediction, the Random Forest model using Random Search CV in SageMaker is the best choice. It achieves high precision, recall, F1 Score, and ROC AUC, indicating strong performance in identifying churn cases while minimizing false positives. This model strikes a good balance between accuracy and completeness in churn prediction, making it well-suited for practical use in customer churn prediction scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
