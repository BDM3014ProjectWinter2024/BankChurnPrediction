{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below if you want to run this file only\n",
    "#%run main.ipynb\n",
    "#%run data_cleaning.ipynb\n",
    "#%run data_visualization.ipynb\n",
    "#%run feature_engineering.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas sagemaker boto3 botocore numpy matplotlib seaborn scikit-learn nbconvert\n",
    "!pip install imbalanced-learn lime shap\n",
    "\n",
    "# Import packages for S3 connection\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import botocore\n",
    "import sagemaker\n",
    "from io import StringIO\n",
    "\n",
    "# Import packages for data manipulation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import packages for data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Import packages for featured engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import packages for data modeling\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
    "f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#SVM\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# This lets us see all of the columns, preventing Juptyer from redacting them.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# This module lets us save our models once we fit them.\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Conection to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3Utils:\n",
    "    def __init__(self, secret_name_or_arn, file_path=None):\n",
    "        self.secret_name_or_arn = secret_name_or_arn\n",
    "        self.file_path = file_path\n",
    "        if file_path:\n",
    "            self.aws_access_key_id, self.aws_secret_access_key, self.bucket_name = self.get_aws_credentials_from_file()\n",
    "        else:\n",
    "            self.aws_access_key_id, self.aws_secret_access_key, self.bucket_name = self.get_aws_credentials_from_secrets_manager()\n",
    "        self.s3_client = self.create_s3_client()\n",
    "\n",
    "    def get_aws_credentials_from_secrets_manager(self):\n",
    "        client = boto3.client(service_name='secretsmanager')\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=self.secret_name_or_arn)\n",
    "        secret_dict = json.loads(get_secret_value_response['SecretString'])\n",
    "        return secret_dict['aws_access_key_id'], secret_dict['aws_secret_access_key'], secret_dict['bucket_name']\n",
    "\n",
    "    def get_aws_credentials_from_file(self):\n",
    "        credentials_df = pd.read_csv(self.file_path)\n",
    "        return credentials_df['aws_access_key_id'].iloc[0], credentials_df['aws_secret_access_key'].iloc[0], credentials_df['bucket_name'].iloc[0]\n",
    "\n",
    "    def create_s3_client(self):\n",
    "        return boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=self.aws_access_key_id,\n",
    "            aws_secret_access_key=self.aws_secret_access_key\n",
    "        )\n",
    "\n",
    "    def check_and_transfer_file(self, source_key, destination_key):\n",
    "        try:\n",
    "            self.s3_client.head_object(Bucket=self.bucket_name, Key=destination_key)\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                self.s3_client.copy_object(\n",
    "                    Bucket=self.bucket_name,\n",
    "                    CopySource=f\"{self.bucket_name}/{source_key}\",\n",
    "                    Key=destination_key\n",
    "                )\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def check_file_exists(self, file_key):\n",
    "        try:\n",
    "            self.s3_client.head_object(Bucket=self.bucket_name, Key=file_key)\n",
    "            return True\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                return False\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def read_csv_from_s3(self, file_key):\n",
    "        obj = self.s3_client.get_object(Bucket=self.bucket_name, Key=file_key)\n",
    "        return pd.read_csv(obj['Body'])\n",
    "\n",
    "    def write_csv_to_s3(self, file_key, df):\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        self.s3_client.put_object(\n",
    "            Bucket=self.bucket_name,\n",
    "            Key=file_key,\n",
    "            Body=csv_buffer.getvalue()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference in connecting on file or secrets manager\n",
    "# Using credentials from a file\n",
    "#s3_utils_file = S3Utils(secret_name_or_arn=None, file_path=\"path/to/credentials.csv\")\n",
    "\n",
    "# Using credentials from Secrets Manager\n",
    "#s3_utils_secrets_manager = S3Utils(secret_name_or_arn=\"YourSecretNameOrARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the S3Utils class from s3conn.ipynb\n",
    "# %run s3conn.ipynb\n",
    "\n",
    "# # Set the path to S3 AWS credentials file\n",
    "# Using credentials from a file\n",
    "#s3_utils = S3Utils(secret_name_or_arn=None, file_path=\"C:/churn/bucketcredentials.csv\")\n",
    "\n",
    "# Using credentials from Secrets Manager\n",
    "s3_utils = S3Utils(secret_name_or_arn=\"arn:aws:secretsmanager:us-east-2:767397996410:secret:dev/s3/bucket_token-6t6xMP\")\n",
    "\n",
    "\n",
    "# Create an instance of the S3Utils class\n",
    "# s3_utils = S3Utils(file_path)\n",
    "# s3_utils = S3Utils()\n",
    "\n",
    "# Define your parameters\n",
    "env = \"dev\" #  dev, test, staging, prod \n",
    "mainsource = \"SourceDataSet/bank_data_train.csv\"\n",
    "envraw = f'{env}/raw/bank_data_train.csv'\n",
    "\n",
    "# Always check the Raw SourceCode is it exists if that it will copy the file to raw on a specific environment\n",
    "# Send sourcefile to env/raw\n",
    "s3_utils.check_and_transfer_file(mainsource, envraw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data from the S3 bucket\n",
    "input_file_key_data_cleaning = f'{env}/raw/bank_data_train.csv'  #this filename is from Kaggle and for the purpose of identification we are going to use the same filename as our main raw datasources. subfolder are used for distinction of file\n",
    "\n",
    "# Output file key directory which is also servers as an input for other process\n",
    "output_file_key_data_cleaning = f'{env}/processed/bank_data_cleaned.csv'\n",
    "\n",
    "output_file_key_data_visualization = f'{env}/processed/bank_data_visualization.csv'\n",
    "\n",
    "output_file_key_data_feature_engineering= f'{env}/final/bank_data_feature_eng.csv'\n",
    "\n",
    "output_file_key_data_X_train = f'{env}/final/model_one/X_train.csv'\n",
    "output_file_key_data_X_test = f'{env}/final/model_one/X_test.csv'\n",
    "\n",
    "output_file_key_data_Y_train = f'{env}/final/model_one/Y_train.csv'\n",
    "output_file_key_data_Y_test = f'{env}/final/model_one/Y_test.csv'\n",
    "\n",
    "# output_file_key_data_model_one_train = f'{env}/final/model_one/bank_data_train.csv'\n",
    "# output_file_key_data_model_one_test = f'{env}/final/model_one/bank_data_test.csv'\n",
    "\n",
    "# output_file_key_data_model_two_train =   f'{env}/final/model_two/bank_data_train.csv'\n",
    "# output_file_key_data_model_two_test =   f'{env}/final/model_two/bank_data_test.csv'\n",
    "\n",
    "output_file_key_data_model_three_train = f'{env}/final/model_c/bank_data_train.csv'\n",
    "output_file_key_data_model_three_test = f'{env}/final/model_c/bank_data_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute notebooks accdly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run data_cleaning.ipynb\n",
    "# %run data_visualization.ipynb\n",
    "# %run feature_engineering.ipynb\n",
    "# %run random_forest.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV From FEATURE ENGINEERING data source file from S3 into a DataFrame\n",
    "# Use the methods from the S3Utils class\n",
    "if s3_utils.check_file_exists(output_file_key_data_feature_engineering):\n",
    "    data = s3_utils.read_csv_from_s3(output_file_key_data_feature_engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.drop('target', axis=1)  # Replace 'target' with the name of your target column\n",
    "y = data['target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Impute missing values in the test set using the same imputer\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scale the imputed test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "X_train_pca = pca.fit_transform(X_train_res)\n",
    "X_test_pca = pca.transform(X_test_scaled)  # Transform the test set with PCA\n",
    "\n",
    "# Train the SVM model\n",
    "model = LinearSVC(kernel='linear', class_weight='balanced')\n",
    "model.fit(X_train_pca, y_train_res)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "predictions_df.to_csv('svm_improved_predictions.csv', index=False)\n",
    "\n",
    "# Save the trained model and preprocessing objects\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump({'model': model, 'imputer': imputer, 'scaler': scaler, 'pca': pca}, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
