{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this part in your code to connect to S3\n",
    "Change the parameters accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the S3Utils class from s3conn.ipynb\n",
    "%run s3conn.ipynb\n",
    "\n",
    "# Set the path to your AWS credentials file\n",
    "file_path = 'C:/churn/bucketcredentials.csv'\n",
    "\n",
    "# Create an instance of the S3Utils class\n",
    "s3_utils = S3Utils(file_path)\n",
    "\n",
    "# Define your parameters\n",
    "env = \"dev\" #  dev, test, staging, prod \n",
    "mainsource = \"SourceDataSet/bank_data_train.csv\"\n",
    "envraw = f'{env}/raw/bank_data_train.csv'\n",
    "\n",
    "#for process data\n",
    "# sourcedata = \"raw\" \n",
    "# targetdata = \"processed\"\n",
    "# sourcefoldername = f'{env}/raw'\n",
    "# targetfoldername = f'{env}/processed'\n",
    "# sourcefilename = \"bank_data_train.csv\"\n",
    "# targetfilename = \"bank_data_test.csv\"\n",
    "\n",
    "\n",
    "# for modeling may vary on environment, and user\n",
    "sourcedata = \"processed\" \n",
    "targetdata = \"final\"\n",
    "model = \"model3\" # model1, model2, model3\n",
    "sourcefoldername = f'{env}/{sourcedata}'\n",
    "targetfoldername = f'{env}/{targetdata}/{model}'\n",
    "sourcefilename = \"bank_data_test.csv\"\n",
    "targetfilename = \"bank_data.csv\"\n",
    "\n",
    "\n",
    "input_file_key = f'{sourcefoldername}/{sourcefilename}'\n",
    "output_file_key = f'{targetfoldername}/{targetfilename}'\n",
    "\n",
    "# Send sourcefile to env/raw\n",
    "s3_utils.check_and_transfer_file(mainsource, envraw)\n",
    "\n",
    "# Use the methods from the S3Utils class\n",
    "if s3_utils.check_file_exists(input_file_key):\n",
    "    df = s3_utils.read_csv_from_s3(input_file_key)\n",
    "    df = df.head(2)  # Extract the first two rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once df is ready for export copy and enable the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Write the extracted data to a new file in S3\n",
    "# Copy this once your dataframe is ready for export\n",
    "\n",
    "#s3_utils.write_csv_to_s3(output_file_key, df)\n",
    "# print(f\"File '{output_file_key}' successfully written to bucket '{s3_utils.bucket_name}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
